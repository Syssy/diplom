Hier kommt alles an wichtigen Todos rein, die vielleicht auch über längere Zeit hier stehen dürfen, aber nicht vergessen werden sollten. Hier wird auch gelöscht, was erledigt wurde

Besprechung:
 * Fragen:
    ** 
 * Berichte:
    **
 * Geplant:
    ** 

Sim, allgemein:
 * Peakfinder: Von Dommi das Sekantenverfahren erklären lassen
 * Sinnvoller Einsatz der Invalid-Flags, bei den Sims sowie PAA

2p-Sim:
 * Vergleich der Peakbreiten bei verschiedenen Längen, wenn das nicht passt: Sättigung einbauen
 * Review/Finalisierung von Peakfinder 
 * Review/Finalisierung von v005
 
3s-Sim:
 * Herausfinden, bei welchen P-Kombis by-step oder by-event besser sind
 * habe beim plottesten noch folgende, tailing erzeugende kombi gefunden: (0.1 0.99) bzw (0.05, 0.99) natürlich extrem früh, aber sollte ich mir merken
 * ReSim-Optionen. Vorbearbeitung der pcombis
 
Referenzdaten: (überflüssig)
 * In VisualNow überprüfen, ob tatsächlich die sehr intensiven Peaks oben abgeschnitten sind, alternativ vergleich mit der handgeschriebenen tabelle
 * Berechnung der Peakdaten komplettieren/Aufschreiben, warum und so
 * Für die Peaks die IQR und IQK bestimmen
 * Zum Kopieren von der Uni: cp boehmer@citrin.cs.tu-dortmund.de:../ims/R06_Zeitspanne_lm/Daten_lm/BD19_1408300548_ims.csv BD19_1408300548_ims.csv

PAA:
 * PAA-Daten und Simulationen vergleichen / Unterschiede berechnen / Ab wie vielen Teilchen hinreichend genau? Existieren Differenzen?
 * Laufzeittests zwischen Julia, Mosdi und Simulationen
 * bei kleinem pmm mal noch größere pll testen
 * Was ist ein Guter schiefewert mit dem koeff? -> ab > 0.01 was zu sehen, richtig schief ab 0.1/0.2
 * Komprimiere Sim-Daten (zb faktor 100), speichere das ganze wieder als .p ab, inklusive peakdaten, plot geeignet verschieben (offset), beschriftung passend, inkl pd
 * review, umstrukturierung, aufräumen
 * Normalisierung/Compression der Daten. Will nicht nur 1 Datenpunkt pro Sekunde, wie speichere ich das sinnvoll ab? Rechne aktuell mit Zehntelsekunden, dann passt das, ist aber ungewohnt
 * Plots zur Flächenabdeckung zu Zeitpunkt

 vor neusimulation:
 pd, params, offset [49.075900000000004, [8.6954, 9.6867, 10.7241], 2.0287000000000006, 0.022723911864740536] [0.4, 0.59995, 5e-05, 0.0019999743, 0.998, 0.0, 2.4974346e-05, 0.0, 0.999975] 39.4801
 nach neuberechnung:
 pd, params, offset [49.075900000000004, [48.1755, 49.1668, 50.2042], 2.0287000000000006, 0.022723911864738787] [0.4, 0.59995, 5e-05, 0.0019999743, 0.998, 0.0, 2.4974346e-05, 0.0, 0.999975] 39.4801
 nach neusimulation ohne offset:
 pd, params, offset [30.001200000000001, [29.0998, 30.0911, 31.1286], 2.0288000000000004, 0.0227720820189234] [0.4, 0.59995, 5e-05, 0.002, 0.998, 0.0, 2.5e-05, 0.0, 0.999975] 0.0

 
Schreiben:
 * Schreiben halt
 * 3-s Modell
 * home/bio/book/unsorted liegen diverse gescannte Bücher. evlt mal da nach statistik literatur gucken
 * Sämtliche Hinweise auf Referenzdaten entfernen, in hypothetische Formulierungen ändern
 * tablesgenerator.com für Tabellen in latex erstellen


 
 