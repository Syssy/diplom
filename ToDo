 
5.5.

Lesen: PAAs, qqPlots ok

Angucken: Heatmaps bzw Kurven

Offen:  T = T1+...+TN mit N zufällig?, was war damit genau gemeint? -> N zufällig anhand der beiden (1, 2, 3 je nachdem) Parameter bestimmen?

Testen: qqPlots ok, jetzt: Ergebnisse des Testprogrammierens gegen InvGauß qqplotten

Brauche: Kurven zum Vergleichen


14.5.
* Physikalische Größen recherchieren (Länge/Durchmesser der MCC, Teilchengröße, Geschwindigkeit, Wechsel zwischen Phasen, Sorption (Kurve?), wie viele Teilchen nebeneinander)
* Gesamtziel: Modelle entwickeln. Verschiedene Möglichkeiten: Ganz einfach mit nur 1 Parameter u 1 Teilchen gleichzeitig, oder zwei Param. 2/3Phasen? Mehrere Teilchen gleichzeitig (wegen Sorption und alle Plätze voll, also Teilchen abhängig voneinander) Wartezeit zu Beginn (->Phys? Wie viele Teilchen/% auf einmal). Konst. Geschwindigkeit (amortisiert sich das nicht über die Länge der MCC?)
* Bis nächste Woche: Modelle sortieren/überlegen, welche es gibt, phys größen

* Formalkram: Expose: Zur Anmeldung nötig, zwei Seiten
* Formalkram: Einführungsvortrag ca 1 Monat nach Anmeldung (Termin rechtzeitig wegen 2. Prüfer)

* Was ergibt sich bei nicht gleichverteilten Zufallszahlen (fürs Hängenbleiben/Lösen) sondern nach anderer Verteilung, macht das Sinn? Oder kann man den Effekt auch erzielen, wenn man eher an den Nachkommastellen von p spielt?


27.5.
* Möglichkeit, die Sim (Counter) zu speichern, um nachträglich noch mehrere Vergleiche anstellen zu können, dabei verwendete Parameter etc. mitspeichern (ok) -> fromfile, tofile, dazu header, wo anzahl etc drin sind


05.06.
* 10000 zz auf ein mal ziehen -> beschleunigen ?!? ok
* kann man bei der hängenbleib-entscheidung was beschleunigen? ok
* Ausgabe der fit-Parameter ok

11.06. 
* Expose anfangen!
* GUI?
* IDE??
* Ausgabe der Stat: Beliebig viele Sim *ok*, bzw. anhand von header-config auswählen hä?
* Gedanken zu meinen files: csv kann man zwar schön lesen, braucht man aber eig. nicht. 
  Vorschlag: Erstelle class wo die Parameter und Zeiten drin sind, das kann ich dann picklen, plotkram kann das dann auch einlesen ok
  Muss noch ein Paket für die Sim erstellen ok
* argparse
* InvGauß: Gucken, was genau mu, loc, scale an den kurven verändern. evtl viele Plot zum vergleich (ok)
* Viele ps/pm Kombis ausprobieren und testen, was da sinnvoll ist. Ausgabe??? 
* Erstelle Liste für WKeiten und ziehe vier paare zufällig, gucken, was das so rauskommt ok 


01.07.2014
* Expose! Evtl mal Felix/Nina fragen ok
* Nach echten Messungen fragen, angucken -> Dommi
* Option für nur plot (-> argparse)
* Sicherung! (git etc)
* Startparameter für fit-Funktion? nö
* GUI: verschoben
 ** Parameter einstellen (Slider)
 ** Speicheroption
 ** Zoom/Achsenbeschriftung für Spektren

 * Schiefe angucken
 -> Heatmaps für Erwartungswerte, Varianzen, Schiefen der Simulationen (für interessante Werte; Ecken, 100 Punkte an je beiden Enden)
 * Erstelle erst Matrix für die Momente, mit leeren Einträgen an den nicht interessanten Stellen. Muss die dann über die params an den geeigneten Stellen wieder füllen. 

*** WICHTIG ***
 Beim Starten größerer Durchgänge unbedingt aktuelles Progamm sichern, sonst nix mehr nachvollziehbar!!!
 Abhängigkeiten sind nicht gut (statsmodels) da es auch auf den Uni-rechnern laufen muss
 
* Wie kann man mehrere eigene Pakete mit Abhängigkeiten "installieren" 
* argparse, damit man den Plotkram auch mit alten Dateien starten kann (teilweise ok)

* Erstelle für die Heatmaps vier subplots für die Ecken, da sonst Werte zu unterschiedlich für schöne plots, plus gemeinsamen Plot ok
* Reihenfolge der pms, pss sollte doch gleich sein, sonst blick ich nicht mehr durch -> egal, wird jetzt sortiert

31.7.
* skewness und kurtosis neu berechnen?!  An der Uni das recalc testen!! Kann ja wohl nicht sein, dass das so unterschiedlich ist. Liegt auf jeden Fall am fit. An der Uni scipy 0.9.0, am Laptop 0.13.3
* Mail wegen batchsystem ok
* Nach Params sortieren und dann plotten, (ok)
* plotkram (4heats) mit kleinerer anzahl sims testen, damit man auch sieht, ob es läuft ok
* Beim Testen, ob Sim schon vorhanden, diese auch in Liste packen, damit am Ende was vollständiges abgespeichert wird! ok
* Wo müssen bei methoden von klassen die selfs dazu?
* Statt erst die Params für die InvGauß direkt die Momente berechnen!! (-> Wie geht das? Scipy kann das!) ok

05.08.2014
* Momente direkt berechnen (ok)
* sim auf snail laufen lassen!! (läuft) Dabei Test, ob Ergebnisse schon vorliegen auf length und count erweitern, Sims davon in Liste (ok)
* Expose
* 4heats hübsch machen joa
* 4 heats mit ein heat kombi ok
* Kommentieren!!!
* Aus großer Menge *.p passende Sims raussuchen und plotten (Param-liste mit kombi von zahl und länge, ähnlich wie bei sim)
* Simulationsklasse vernünftig in plotkram einbinden
* Warum zur Hölle sind plötzlich die Plots nicht mehr richtig sortiert??? (Liegt an der anderen Sortierung der .pickle's als der .p's. Achsenbeschriftung ist auch irreführend) ok
* Vorbereitung auf Treffen: Einzelne Plots angucken ok
* Option für recalc, irgendwie das recalc auch abspeichern (ok -> in plotkram, die Option rc)
* Für Fakesims mean/variance nicht = 0 damit das mit dem log-Plotten klappt

nächstes Treffen 4.9. 16 Uhr

-m cProfile  -o sim.prof  sim.py ->  gucken, wo ich die meiste Zeit brauche

Aufschreiben: 2 Params scheinen nicht zu reichen -> Done
Geschwindigkeitsparameter.
Wie oft bleiben die teile hängen. sim: auswürfeln, wann wieder was passiert, statt jeden zeitpunkt zu betrachten ? (Lohnt das? -> cProfile)
Dommi nach Daten fragen (spektrum mit guten params und wenigen Peaks) ok
Simulation einer Multikapillarsäule bei der Ionenmobilitätspektrometrie

14.08.2014
* Dommis File angucken, antworten (ok), vor allem noch mal ne einzelne Zeitspalte angucken, evtl. schauen, was scipys fit damit macht, qq-Plot (ok, aber hier bin ich noch nicht fertig)
* Testen besser machen (zu viele unübersichtliche ifs ob schon vorhanden) ok?
* Aufräumen: Universionen mit dem hier synchronisieren. Unterordner für Länge und Anzahl, .pickles nach parameterbereichen benennen ok
* Expose (bin dran...)
* Einzelne Sims samt Histogram, fit-Plot und qq abspeichern und gut kommentieren

* testen: pm>ps sollte für schmalere peaks sorgen, daher das mit langsamerer vel kombinieren

* Was passiert wann - Simulation: Auswürfeln, wann wieder was passiert, statt jeden Zeitpunkt: 
 ** Wie bestimme ich den nächsten Zeitpunkt? Erst mal gleichverteilt, um zu gucken, ob das überhaupt sinn ergibt s.u.
 ** Wo speichere ich den ganzen Kram ab? Kann ich dafür auch np-arrays nutzen oder muss ich das einzeln machen?
 ** Mache Liste für jeden Zeitpunkt, wo alle relevanten Teilchen drin sind.
 ** Pro Teilchen speichere Ort und Zustand (Geschw... etc)
 ** Teste auf fertig? -> bei jeweils jedem Schritt 
 ** Wie würfele ich den nächsten Zeitpunkt richtig aus? (Also Params einbauen!) ok
 ** Ergebnisse plotten
 ** Verteilung für nächsten Zeitpunkt? -> Geometrisch (bestätigt)
 ** KOMMENTIEREN ok
 ** Profiling davon
 
04.09.2014
 * Dommis File: Plotten ohne Histogramm, sondern Punkte verbinden
 * p = 1/v * gamma; pm=ps? Damit es nicht noch mehr Params werden -> Ausprobieren
 * Dummerweise hab ich nicht mehr so ganz in Erinnerung, was genau beim 4.9. gesagt wurde :( Ich sollte nur das mit den Geschwindigkeiten weiter ausprobieren und rausfinden, ob das ein guter Ansatz ist.
 * Finde sinnvolle Werte für v und gamma, sodass p immer noch nahe 1 ist
 * Außerdem soll v sich ändern können, insbes. nach Wechselwirkung
 * Evtl in die wpw einbaubar: in der sim brauche ich nur einen param, anhand dessen bestimmt wird, wann wieder was passiert und halt die vel, die mir sagt, wie weit ich gehe. Das evtl. neu-setzen des params dann in extra methode? Nee, hier nicht einbaubar, da Geschwindigkeitsänderungen... Oder doch, wenn man einfach die Durchschnittgeschwindigkeit über den Zeitraum annimmt?
 * Interessant wären durschnittswerter für vel und p 
 * Zusammenhänge zwischen den Params herausfinden, welche Kombis sind sinnvoll? Effekte der einzelnen auf die Sim?
 
 29.09.2014
 * Sim002 mit Wechselw'keit anfangen
 * Gedankenspiel... Gehe noch mal zurück zu den Modellen 1a/b: In b ist p die W'keit im nächsten Schritt mobil zu sein. Also immer gleich, unabhängig vom aktuellen Zustand. Dadurch gibt es aber schon im Ansatz den gewünschten Effekt: Je größer p, desto weiter vorne und schmaler ist der Peak.(Allerdings kein verstärktes Tailing bei den hinteren Peaks, nur insgesamt flacher)
 Außerdem widerspricht das der aktuellen Überlegung, dass die W'keit geschwindigkeitsabhängig ist. Neuer Gedankengang... 
 Die W'keit mobil zu sein, hängt nur von der aktuellen Geschwindigkeit ab (mit irgendeinem Faktor). Bin ich normal-schnell gibt es halt eine gewisse Wkeit hängen zu bleiben. Bin ich schneller, ist diese geringer. Wenn ich schon hänge, ist sie sehr groß (weiter hängen bleiben realistisch)
 
 * Achtung: Länge der Säule anpassen an Geschwindigkeit, sonst bekomme ich wieder den führungspeak. Außerdem sinnvolle Werte für den veldivisor mit einbeziehen, da das die maximal (tatsächlich) erreichte vel doch sehr verändert
 * Vom Gefühl her ist es besser, ps weiter als (festen) Parameter zu haben.
 * Berechnung der neuen Geschwindigkeit muss evtl noch angepasst werden. Keine Ahnung, was da realistisch ist. Bisher wird einfach der veldivisorste Teil der Differenz zwischen aktueller und maximaler Geschwindigkeit genommen. Dadurch werden langsame Teilchen mehr beschleunigt, als schnelle. Vielleicht ergibt aber auch was anderes mehr Sinn.
 * Es gibt grad so viel auszuprobieren... Das ist auch gut, aber bitte dokumentieren. Oder wenigstens die Bilder abspeichern... (ok)
 * Gleiches für die älteren Versionen, dann hab ich schon mal was fürs zusammenschreiben
 * Versuche Wissen von Figure1 zu nutzen...
 * Warum habe ich die Idee, ps auch von gamma abhängig zu machen verworfen? Da vielleicht noch mal Gedanken zu machen
 * Es sollte geklärt werden, was an festen Parametern sinnvoll ist. Also velmin/max und evtl. ps. Das scheint im Moment recht willkürlich zu sein.
 * Es wäre hübsch, wenn die Geschwindigkeit zb Normalverteilt wäre (also Kurve mit Maximum) und nicht hauptsächlich an den Rändern. Dann könnte man durch geschickte Parameterwahl dieses Maximum hin und her schieben. -> Noch mal bei Figure1 gucken, da war die zweite Kurve ja im Ansatz so, vielleicht bekommt man das mit nem anderen maximalwert ja hin? So spontan eher nicht. 
 Sollte das nicht gerade durch das ganze Hängen-bleiben erreicht werden? Tut es ja auch, aber leider wird die Varianz dabei zu groß
 * Fronting bei Figure1 zu klären? Gab es das da? Ist ja auch ein Effekt von hohem ps
 
 * EXPOSE: Hätte gerne irgendwelche Bilder, an denen man das mit dem Tailing und der Varianz gut sehen kann
 * EXPOSE: Noch mal Literatur raussuchen, evtl querlesen. V.a. was zur MCC/ GC
 
 * Kann man die Geschwindigkeit einfach weglassen und nur mit variablen Wahrscheinlichkeiten arbeiten? Müsste im Endeffekt auf's gleiche rauskommen, aber man könnte zwei Parameter weglassen) Das könnte vor allem bei der wpw-Sim einbaubar sein
 
10.10.2014
* Echte Daten raussuchen, Einheiten festlegen: Zeitschritt = 1/1000s (oder später 1/10000) Messung nur alle 1/10s (real). Geschwindigkeit raussuchen
 (40cm/s bei 30-60s pro Messung und 4-25cm Länge) In der Besprechung war von ca 10min pro Messung die Rede, wie passt das denn nu?
* Mit dem 2-Param-Modell noch mal einige Param-Kombis testen und gucken, wie man die Peaks so schmal bekommt
* Breite des Peaks bei halbem Maximalwert; Gucken, wie das mit steigender Zeit ist. (Verhältnis Peakmax zu 1/2Peakmax) Dazu:Dommi anschreiben ok
* Dann gucken, bei welchen p/q (pm/ps)-Kombis das hinkommt (plotten), evtl könnte man da eine Funktion für finden
* PAA-Ansatz, die Formel angucken und gucken, wie sich das berechnen lässt. Eigentlich ja "nur" ne einfache Rekursion, muss nur sehr auf den Speicher achten

* Den Berechnungsansatz kommentieren!!

* Aus dem Gespräch mit Dommi:
 ** Die Breite bei halbem Maxwert hat er nur für IMS-Messungen, da gibt es ne Berechnung, die Faktoren der IMS wie Gitteröffnungzeit mit einbezieht, das kann man nicht einfach so auf die MCC übertragen. Da scheint die Breite unabhängig zb von der Injektionszeit zu sein
 ** Um die Peaks der MCC zu bekommen, müsste ich von den Spektren den RIP rausrechnen, da der ja nicht durch die MCC läuft und zusätzlich die Gerätefunktion rausrechnen, da die auch erst durch die IMS reinkommt. Alternativ: Gibt es reine MCC-Daten?
 ** Messungen unter home/ims gut sind die 6er/7er gemische, KK-Daten. Bei den Pseudos sieht man gut die Gerätefunktion
 ** Er hat mir zwei Paper-links geschickt und was zum Einlesen der Daten. Ein Programm, um die Gerätefunktion zu berechnen gibt es auch
 ** Jetzt erst mal rausfinden, was ich wirklich davon tun soll.
 
16.10.2014
 * Echte Daten: In den vorhandenen IMS-Messungen gucken. Nur wo bitte soll das drin stehen?
 * Für die halbe-peak-breiten-berechnung einfach eine spalte der Messung nehmen, was Dommi sagte, ist nicht so wichtig, da ich ja auch nur einen Peak sehen will und nicht das ganze Chromatogramm
 * fsolve macht nicht so ganz das, was ich gerne will, manchmal wird nur ein Schnittpunkt gefunden oder auch einer, der nicht da ist. Muss wohl an den start estimates liegen
 * Sollte jetzt die siebener-Gemische angucken und die Werte ermitteln
 * Größeres Projekt könnte sein, die Zusammenhänge zwischen Parametern und den Breiten zu ermitteln. Mir fehlen leider immer noch die echten Werte
 * Zeitskala sollte bis 600 gehen; vor 50s ist schlecht was zu erkennen; Peaks mit wenig Tailing sollten dabei 25-30s breit sein, krasses Tailing geht dabei schon mal übers halbe Bild
 
 
 30.10. 2014
 * Fragen: Wo krieg ich nun die echten Daten her? Ist das Peakbreiten-Plotting so ok?
 * Im Breitenplot: Parameter so raussuchen, dass eine gerade entsteht, hoffentlich passiert das mit einem bestimmten zusammenhang zwischen den dafür nötigen params. Dann im skew-plot gucken, ob da auch ein zusammenhang existiert
 * Was tue ich, wenn ich jetzt so eine gerade gefunden habe?
 * Gerade auch bei festen ps und variablem pm?
 * Spektrum plotten für "viele" ok
 * Jetzt endlich aus den echten Daten die Breitenverhältnisse raussuchen (dafür brauch ich aber schon nen ganzen morgen... mindestens...)
  ** Ideen dazu: 
  ** Im Wesentlichen eine Messungsklasse, die genau das hat, wie meine Sim
  ** Dazu: Einlesen einer einzelnen Messung. Hier stehe ich wieder mit nem Brett vor dem Kopf da: Muss ich das jetzt für nen ganzen Peak (sprich muss ich da mehrere Spalten aufaddieren) oder reicht tatsächlich eine Spalte? EIGENTLICH (denke ich zumindest) müsste ich das alles aufaddieren und den Kram, den Dommi erwähnt hat, rausrechnen. Dann kann man meinetwegen immer noch einen einzelnen Peak raussuchen und den alleine betrachten. Sollte ein Analyt nämlich in der IMS-Dimension sehr breit werden, sehe ich im MCC-Spektrum möglicherweise nicht, WIE hoch er wirklich war. Zumindest sollte ich das bei der Auswertung berücksichtigen, dass IMS-breite Peaks evtl nen viel höheren Wert haben, als ich sehe.
  ** Weiteres Problem: Reicht da die Annäherung als Gauss? Dazu: Plots angucken!
  ** Aus den Zeiten ebenfalls die plots erstellen bzw die breite berechnen
  ** Zum Vergleich: Tabelle anlegen und gucken, welche Params von mir da hin passen (bisher gehe ich ja eher den umgekehrten Weg, nur meine Sachen anzugucken)
 * Problem: Die ersten Peaks fangen erst bei 100 an. In echt geht das aber schon viel früher (da ist quasi sofort was da). Dafür müsste ich meine Skalen aber anpassen, zb um Faktor 10. Aber dann hab ich auch keine Peaks mehr bei > 60... Also mal wieder das bekannte Problem, dass ich die Dinger nicht genug getrennt krieg. Alternativ müsste ich krasse Params wählen (ps=0.1 und pm = 0.9, dadurch wären die aber keine Peaks mehr mit Breite, sondern alle quasi gleichzeitig da)
 * Bei d3: Bei einem plus, bei einem minus?
 
 06.11.2014
 * Noch mal nachrechnen, ob das mit den 2km/s so stimmt ok... stimmt nicht
 * Gucken, wann die ersten brauchbaren Peaks in den Spektren erscheinen... Wird da echt nur zwei Mal pro Sekunde gemessen?
 * Zeiteinheiten festlegen
 * Simulation mit pm = 1 machen, sodass irgendwas zwischen 10⁻⁴ und 2s raus kommt (ersteres müsste für die 2km/s sein, letzteres sind die ersten halbwegs brauchbaren Peaks) -> Korrektur auf irgendwas zwischen 0.1s und 2s
 * Simulationen dann erst mal (mit den gewählten Einheiten) für zwei Minuten (statt bisher 10, also 120s) laufen lassen und gucken, ob man da brauchbare Params findet; korrektur auf vier Minuten
 *"irgendwas" gab es, dass ich im Endbericht nachlesen könnte, weiß aber nicht mehr, was das war :( Vielleicht sollte ich den einfach abends noch mal durchgucken
 * Beim Spektrum plotten irgendwie einen Farbverlauf einfügen, damit man nachvollziehen kann, welche Farbe welcher Plot ist (joa)
 * Beim Spektrum die gesamtskala auf 0..240 festlegen. ok, zusätzlich die höhe auf 0..1 festgelegt, das muss aber evtl noch geändert werden 
 * Bekommt man vorne auch breitere Peaks hin?
 * Heatmap für die Breiten erstellen. ok, jetzt nur noch sinnvolle pkombis für die plots auswählen
 
13.11.2014
 * Expose fertig machen!!!
 * Aus den 6er/7er Gemischen die Breiten bei halber Höhe heraus finden
 ** Dazu: Mit Visual Now anzeigen lassen (brauche den Plot rechts, Bildschirm drehen ;))
 ** Muss nicht 100%ig genau sein
 * Diese Peaks simulieren (also passende Params finden) Versuche also, Kombinationen zu finden, die bestimmte Breite an bestimmter Zeit erzeugen

 * Peakfinder: 
  ** Starte mit beliebiger Param-Kombi bzw suche in den bisherigen Sims nach guten Annäherungen
  ** evtl. davon eine auswählen, mit der weiter gesucht wird -> User Input nötig
  ** Je nach Lage und Breite Params verändern
  ** Brauche dafür genauere Infos, wie die ps und pm die Shapes beeinflussen (Aufschreiben, wo genau ist der Zusammenhang)
  ** Alternativ: Wenn nah dran, einfach viel in der Umgebung suchen
  
 * Das alles löst nicht das Problem mit dem Tailing
 
20.11.2014
 * map, imap, processing angucken, evtl ist das sinnvoll für mich; joa
 * Habe das Gefühl, wenn ich manuell vier mal was starte, dass das wesentlich langsamer ist, als wenn ich nur drei laufen lasse. Das sollte ich evtl testen (gucke nach gesamt/einzelzeit für 3*4 oder 4*3 Simulationen). Dann würde sich das multiprocessing evtl auch anbieten, s. Done
 * Evtl lohnt es sich, noch mal die wpw-Sim auszupacken, die war (wenn ich es richtig in Erinnerung habe) viel schneller, wenn nur selten was passiert. Zumindest für Sims mit mehr Teilchen, damit ich ne schöne Ausgabe habe, die ich später verwenden kann, würde sich das evtl lohnen;
 * Auch interessant wären so Dinge wie: Ist es schneller, wenn die simulationsmethoden direkt im Programm stehen oder über die Simulationsklasse aufgerufen werden??
 * erstelle Plots, auf denen für bestimmten Zeitpunkt zu sehen ist, welche Params einen solchen Peaks erzeugen und welche jeweilige Breite damit erreicht wird (siehe Zettel)
 * Für diesen neuen Plot:
  ** Ausgehend von mehreren Kombinationen in der Nähe alles weiter eingrenzen (joa, ist momentan manuell machbar)
  ** Möchte ausdrücklich mehrere Kombinationen haben, daher evtl aus der anderen Richtung kommen und für feste ps oder pm (was ist sinnvoller) den jeweils anderen Parameter annhähern, bis ich gut genug die Zeit treffe
  ** Wie genau muss das eine Retentionszeit treffen? -> Ergebnisse, man erkennt ganz gut eine Gerade bei epsilon = 0.1
  ** Peakfinder automatisch annhähern lassen! hmm...
  ** Starte mit mehreren verschiedenen ps, für die passende pm gesucht werden. Es reicht ja, jeweils den ein klein wenig anzupassen und zu gucken, ob der Peak in die richtige Richtung wandert. Da die Breite ja egal ist, kann einfach der zweite Parameter optimiert werden, wenn entweder die Zeit erreicht ist, oder die Änderung zu gering -> Abbruch
  ** Es wäre doch sinnvoll, beide Params zu verändern (möglicherweise aber auch nicht gleichzeitig) da sonst mehrere mit gleichem ps (pm) und verschiedenem pm (ps) die Zeit beschreiben.
  ** Brauche, damit es präzise wird, Sims mit 10000 Teilchen. Dafür noch mal die wpw auspacken, da manchmal schneller
 * Aufschreiben, was bisher erreicht wurde, wo die Grenzen sind, was nicht geht etc. 
 * Es wäre schade, wenn das das einzige Ergebnis bliebe... (Aber das heißt, dass es immerhin schon ein Ergebnis ist)
 * Vorher erst mal Expose fertig machen, jetzt aber wirklich; Ich bin ja dran...
 * Code aufräumen, ordentlich dokumentieren und ne "finale" Version der 2-Param-Sim erstellen
 
27.11.2014
 * Literatur für's Expose raussuchen und einbinden
 * Auf Rückmeldung warten, wer genau den Zweitbetreuer macht
 
06.01.2015
 * Expose überarbeiten (joa, s.u.)
 * Tau-Leap Methode angucken (?, fällt doch raus)
 * Links (Mail vom 12.12) angucken (ok)
 
 * Im Expose: 
  ** Titel ändern, IMS weglassen
  ** Literatur einheitlich, (Journals groß, rest klein, auch wenn im Orig anders)
  ** Statt tau-leap: waiting time/wartezeit
  ** Andere Arbeiten erwähnen, die !nicht! das gleiche tun, wie ich (hplc/spreadsheet etc.) Dazu evtl genauer rausfinden, das hplc tut, paper gucken (Steht drin: "based on experimental measurements", also keine echte Simulation in unserem Sinne)
  ** Literatur im text vernünftig zitieren
  ** Halbe-Peakhöhen-Breite als Vergleichsmaß erwähnen, dazu schreiben, wie viele Datensätze zum Vergleich vorliegen
 * Sachen fürs Prüfungsamt raussuchen und hingehen (Do geht nicht; Alte Ausdrucke, wegen DVEW)
 
 * Zitieren: Wie krieg ich beim Buch das "Auflage" weg
 

15.01.2015
 * Numba
 * Expose überarbeiten
 * Prüfungsamt
 * Evtl schon mal Folien anfangen, Dinge aufschreiben
 * In der Bib: (Säulenbluten) Adsorptionseffekte verursachen Tailing
 * Stoffe wie amines, alcoholes, carboxyl etc prone to tailing wegen hydrogen bonding (Genauere Ursachen?)
 * Um Tailing festzustellen: Peakbreiten in 10% der Höhe messen. 
 * Für Halbwertsbreite evtl noch Basislinienhöhe abziehen? Damit müssten die gemessenen Breiten größer werden, da die Höhe abnimmt und Peaks auf der Hälfte breiter werden
 * Noch mal an Dommis Rechner -> hatte ich für die Höhen die Basislinie abgezogen? (ja, hatte ich, mehr oder weniger)
 * Was soll zum PAA ins expose? 
 

29.01.2015
 * Doch noch nen Satz mehr ins Expose zum PAA. Kurz erklären, was er tut und Entsprechungen für Zustände, Werte und Emissionen
 * Mosdi angucken, PAA damit basteln.
 * Da das Berechnen mittels PAA doch recht lange dauert, vielleicht wäre es sinnvoll, das Ganze in Teilprobleme zu zerlegen...?
 * Dazu müsste man in der stateValueStartDistribution was anderes angeben. Spontaner Gedanke dazu wäre es, dazu vorher x Iterationen zu machen, möglicherweise mit der doubling technik, bis zu in etwa der zeit, wo es interessant wird (oder x=länge) und dann noch mal als neue max-zeit die erwartete Breite des peaks anzugeben. Dann muss man zwar das ganze zum vergleichbaren plotten noch verschieben, aber das sollte hinkriegbar sein. Den Gedanken sollte ich in Ruhe und auf Papier noch mal überdenken, dazu am Besten noch mal mit 10 Iterationen oder so in der Sim angucken und vor allem überdenken, ob das mit der stateValueStartDistribution so passt.
 * Gibt es ne sinnvolle möglichkeit das zu zerlegen? die doubling-technik nützt mir wohl nix. Ob das Zerlegen was bringt, glaube ich nicht mehr, da es linear in den Schritten und values ist (ohne die doubling). Könnte aber doch was bringen, falls sonst die Arrays zu groß werden
 * gnuplot angucken und versuchen, da was aus java reinzupipen

 
05.02.2014
 * Expose: Ausdrücklich schreiben, dass zwischen Modellparametern und Peaks (mit Lage, Breite, Schiefe) hin und her gerechnet werden soll
 * PAA: Gucken, ob ich das (mit Julia) nachprogrammieren könnte und dann nicht mehr auf 2D double-arrays gearbeitet wird, sondern irgendein Interface möglich ist, was zwar mit 2Dd-a implementiert werden kann, aber nicht muss, um die nicht verwendeten Teile rauszuschmeißen. Ein Großteil ist ja noch nicht erreicht und daher null oder schon bis auf minimale Restw'keiten geleert.
 * Anfangen zu schreiben/Vortrag
 * Brauche Möglichkeit, die vom PAA berechneten Wartezeiten zu speichern. Da hier ja auch nie mehrere Durchläufe nötig sind (kommt bei gleicher Eingabe immer das gleiche raus) lohnt sich das. Sinnvoll wäre wahrscheinlich eine csv
 * paa-klasse anpassen, dass die parameter einstellbar und die größen anpassbar sind
 
 
11.02.2015
 * Vom PAA berechnete Daten exportieren; auf die Werte normalisieren (Einheiten)
 * Langfristig: PAA-Daten und Simulationen vergleichen / Unterschiede berechnen / Ab wie vielen Teilchen hinreichend genau? Existieren Differenzen?
 * Zu diesem Zweck wäre es evtl. günstig, die bisherigen Histogramme auch in Wahrscheinlichkeiten umzurechnen, die dann als einfacher Plot darstellbar sind. Oder anders herum, irgendwie die Wartezeitverteilung zu konkreten Daten umzuformen (was aber weniger Sinn machen würde, da dort Rundungfehler beim Umrechnen auf ganze Zahlen auftreten würden?)
  -> In Python: Bei plt.hist (bzw dahinter np.histogram) über normed/density wird das auf 1 normalisiert, sodass sich W'keiten ergeben
    >>> a -> array([2, 3, 3, 4, 4, 4, 5, 5, 6, 7]) # Minidatensatz
    >>> hist, bin_edges = np.histogram(a, bins = 6, density = True) # a: Daten; hist, bin_edges entspr. n, bins (patches) bei plt
    >>> hist -> array([ 0.12,  0.24,  0.36,  0.24,  0.12,  0.12])
    >>> bin_edges -> array([ 2., 2.83333333, 3.66666667, 4.5, 5.33333333, 6.16666667, 7.])
    >>> b = hist * np.diff(bin_edges) # Berechne Wkeitsvert
    >>> b -> array([ 0.1,  0.2,  0.3,  0.2,  0.1,  0.1])
    >>> plt.plot(b)
    >>> plt.show()
 * Dringt der Analyt in die stat ein oder bleibt er dran kleben? (-> Vortragsgraphik)
  ** Adsorptions -> Ablagerung an der Oberfläche durch van-der-waals-kräfte oder Wasserstoffbrückenbildung (stärker)
  ** Verteilungs -> Lösung in den Phasen
  
  
19.02.2015  
 * Mail an Rahnenführer (Termin Vortrag ab 02.04.)
 * Schreiben: Modell -> Wie beschreibe ich mein Modell formal. Am Besten erst mal auf Papier...
 
 
16.03.2015 
 * Mein Plan für die nächsten Wochen:
  ** PAA in Julia implementieren
  ** Aufschreiben:
   *** Grundlagen
   *** Modell
   *** Implementierung
   *** Stichpunkte zu Evaluation/Diskussion
  ** Implementierungen angucken und eine "Final"-Version erstellen, die gut kommentiert ist
  ** Drei-Parameter-Modell erstellen, implementieren (erst nach dem review der alten versionen)
 * Review:
  ** Nötige Dateien:
   *** IMS_Dommi/peak_width.py abh: ims_core.py
   *** Param_p/2-Param-Modell/peakfinder.py /2_param_v005.py (evtl. parallelv005?)
   *** Param_p/Wannpassiertwas/ ? (->simulation)
   *** Pythonkram/Plotkram/plotkram.py 
   *** Pythonkram/Simulationsklasse_2Param/simulation.py
  ** Aufruf von 2_param_v005 startet Simulationen, über Kommandozeilenparameter bestimmt, welche
  ** Zusätzlich neues Script, welches Plots ausgibt (Plottings?) und eines, das Berechnungen anstellt und diese dann ausgibt.
  ** Peakfinder kann 2_param_v005 aufrufen, mit pcombis
  ** Überlegung für die ganzen Plotsachen: Aufruf geschieht ja entweder from_file oder mit Daten. Problem: Dateien manchmal unsortiert, was für das Plotten doof ist. Erstelle also jeweils alles mit funcxy_from_file zusätzlich, welches die Daten extrahiert und funcxy aufruft
  ** peak_width fliegt raus; effektiv nur calculate_width (ehemals fpwahph) genutzt, das zieht um in die sim-Klasse. Plot-Dinge sollten entsprechend ins my_plottings umziehen  
 * Mal von nem anderen Rechner testen, was bisher so da ist  
 

26.03.2015
 * ist es besser array leer anzulegen und zu pushen oder vorher länge festzulegen und über indexzugriff zu setzen?
 * Alternatividee zu den sparse matrices: Einfach Liste speichern, da die Werte ja in einem zusammen hängenden Bereich sind. Jeweils den ersten Wert testen, ob er noch über einem Grenzwert ist, falls drunter, eine Index-Variable erhöhen und die erste Stelle löschen
 * Klären, ob allgemeiner PAA gewünscht ist, oder die spezifische Variante reicht
 * WaitingTimeForValue
 * Zeitanalyse: Am meisten Zeit scheinen push! und unshift! zu benötigen, keine Ahnung, wie ich das umgehen kann. Alternativ könnte ich doch mal eher den PAA aus mosdi nachprogrammieren, glaube aber kaum, dass das was bringt

09.04.2015
 * Bei den Peaks noch mal die Breiten überprüfen, dabei Basisrauschen beachten (dadurch werden die vielleicht schmaler, wenn Rauschen abgezogen werden muss) ok -> nein, leider!
 * Screenshot / jpg-Export von den Visual Now - Ansichten
 * Welche Ergebnisse sollen in den Vortrag?
 * Noch mal einen Plot wie in 7a erstellen, aber mit je 10000 Teilchen.
 * In der simulation noch mal nachrechnen, welche Einheiten nun korrekt sind
 * Muss ganz dringend den PAA mit dem PAA überprüfen, welche Zeiten jetzt realistisch sind. Habe ein Faktor10 Problem -> Plotten
 
23.04.2015
 * Plots per Inkscape nachbearbeiten für Lesbarkeit
 * Markoff-Ketten erwähnen
 * Sven vorher Folien zeigen
 * Zahlen auch in den Plot zur Lesbarkeit trennen
 * Ausprobieren in python: r"p_s" oder so ähnlich. Noch mal Henning/Nina fragen
 * Cooles "Video", wie die Teilchen da durch wandern (zu mehreren Zeitpunkten als Bild ausgeben und zusammen schneiden)
 * Die neuen Modelle umsetzen, vor allem theoretische Überlegungen, zur Berechnung. Zustände nicht mehr einfach als true/false darstellbar
 * Gedanken zur Adsoptions-Sättigung machen, wie ist das einbaubar?
 * In VisualNow überprüfen, ob tatsächlich die sehr intensiven Peaks oben abgeschnitten sind
 * Gedanken machen, ob nur der Maximalpunkt, oder Intervall betrachtet werden soll
 * Baustellen sortieren
 * Weiter schreiben, besonders Dinge, die ich im Vortrag erwähnt habe
 
 * Im Mom gehe ich ja jeweils 200 Einheiten vor, das entspricht 1000 Einzelschritten bei einer Länge von 200000. Wenn ich das auf ne kleinere Schritteinheit umstelle, muss ich auch die Zeiteinheit anpassen, d.h. bei je eine Einheit pro Schritt nicht mehr durch 10000 Teilen, sondern durch 2000000
 

30.04.2015
 * Zur 3-Zustände Simulation: Erweiterbar machen durch 3er-Array, z.B. 0-0-1 für Vorwärtsschritte und Matrix für Übergänge. Zustände dann als 1-2-3 abspeichern
 * Habe jetzt zwar die Einheiten in der _2p so festgelegt, dass step und event das gleiche liefern, muss aber noch mal viele Sim laufen lassen und mich dafür auf eine Schrittweite festlegen. Momentan liegt die bei 100. Bei der neuen Schrittweite dann auch noch mal alle pd überprüfen, ob die auch gleich sind. Alternativ die Einheiten auf Teilchenebene festlegen, dass da keine Zahlen mehr fest in den Methoden stehen.
 * 3-states: Versuchen die by-event (_simulate_event) zu parallelisieren. Das lohnt sich aber nur, wenn viele Events zum gleichen Zeitpunkt statt finden (Params nicht zu nah an 1, dh events in kürzerern abständen), wahrscheinlich lohnt an gleicher Stelle eher eine gute Implementierung für die by-step Variante. 
 * by-event lohnt sowieso nur bei extremen wkeiten, da diese aber momentan parallel mit mittleren wkeiten vorkommen, lohnt evtl doch eine parallelisierung
 
 * Sollte jetzt erst mal die 2p fertig überarbeiten, vor allem brauche ich demnächst wieder den peakfinder, gerade bei 4/6 Params wird es bald unübersichtlich.
 * seaborn (zum plotten) evtl paket nachinstallieren
 

07.05.2015
 * PAA
 * Experimente starten (ok) und Ergebnisse auswerten.
 * Gute Idee, wie ich die Auswertung mache! Wie finde ich heraus, welche Paramkombis gut sind? 
  ** Tatsächliche Daten raussuchen, gucken, ob ich da annähnernd dran komme. 
  ** Werte für die Schiefe, bzw rechts und linksbreite berechnen
  ** Überprüfen, wie ausagekräftig diese sind
 * Wenn ich ans Annähern der Peaks kommme: Sekantenverfahren, Dommi noch mal ansprechen
 * Brauche klare Werte, wann ein Peak "gut" ist
 * mit np hist die bins berechnen. davon offset für die mitte der bins und damit plotten (Mail von Bianca) ok
 * in den Plottings einige methoden überarbeiten, da zu unflexibel (vorgegebene quadratische anzahl etc) ok
 * [0.5, 0.4, 0.1], [0.0095, 0.99, 0.0005], [0.0003, 0.9996, 0.0001]] in der 3s(6p) T und E vergleichen
 * habe beim plottesten noch folgende, tailing erzeugende kombi gefunden: (0.1 0.99) bzw (0.05, 0.99) natürlich extrem früh, aber sollte ich mir merken
 * Schiefe der echten Daten berechnen 
 * Noch eine Idee für den Plotkram: grundsätzlich nur noch ohne fromfile starten, aber ins main und über argparse die möglichkeit einbauen, dateien zu öffen zur direktbenutzung
 * Problem bei der Berechnung der Schiefe: In den Referenzdaten: Bis wohin geht ein Peak, was gehört zum Rauschen? Wie rechne ich das raus? Oder eher doch die Breite auf halber Höhe rechts und links des Maximums nehmen? Diese Daten liegen für die Referenzen vor, müsste ich für meine Daten berechnen.
 * Muss sowieso die Berechnungen (calculate) komplett anpassen, da das alles auf Normalverteilung beruht.
  ** Wie berechne ich das Maximum und die Maximalstelle?
  
21.05.2015  
 * In den Referenzdaten ein Histogramm über die Intensitäten machen. Das Rauschen am Anfang soll (in etwa) eine Gaußkurve bilden, das sind dann die Werte, die rausfallen, bzw, den höchsten Wert (gespiegelt von der ersten Hälfte) ziehe ich dann von allen Werten ab. Die Schiefe berechne ich dann über alle Datenpunkte ausgehend vom Maximum bis zum ersten erreichen von 0. Sollten das zu wenige Datenpunkte sein, auch jeweils das Nachbarchromatrogramm nehmen.
 * Was tun bei mehreren Peaks im Chromatogramm? Lokale Umgebung macht schon Sinn, da zb in der Nähe des RIP die Rauschwerte deutlich höher sind, als sonst. Auch starkes Tailing verursacht höhere Rauschwerte, sodass das tatsächliche Ausmaß des Schwanz nicht erkannt wird. (Optisch aber sofort zu erkennen)
 * Terminklärung zum Treffen mit Rahnenführer (ok)
 * Ideen für die Verschiebung der Peaks nach vorne bei gleicher Breite: Sättigung der Adsorption mit einbeziehen. Überlegung, ob das dann beim 3s Model, auch auf Lösungszustand zutrifft oder wie da halt die Abhängigkeiten sind. Dabei müsste für jeden Ort/Ortsbereich eine Sättigung in jedem Schritt berechnet, gespeichert werden und davon abhängig die Wkeit zum Übergang verändert werden. Das erhöht im Prinzip pm (wodurch Peaks ja schneller werden) Wenn dabei die Breite gleich bleibt, wäre das super. Möglicherweise müssen dafür aber viel mehr Teilchen simuliert werden, um einen sinnvollen Wert für die Sättigung zu erreichen?! Umsetzung als eventsim stelle ich mir schwer/unmöglich vor, da dort vorhersage der Sättigung nötig wäre. in der timestepsim aber würde ich einfach zweischrittig arbeiten, alle übergangskandidaten noch mal auf zweite wkeit testen, ob sie hängen bleiben können.
 * Könnte noch mal gut etwas Grundlagenliteratur brauchen, um abzuklären, was für Ideen jetzt realistisch sind.
 * Vorgehen bei der Schiefebestimmung: 
  ** Finde Maximalwert und Maximalstelle des Peaks
  ** Erzeuge Wertehistogramm des Chromatogramms
  ** Finde darin Maximalwert / "Gausskurve" zur Bestimmung des Rauschlevels
  ** Nehme vom Peak, ausgehend von der Maximalstelle, alle Werte bis zum ersten Erreichen des Rauschlevels (in beide Richtungen)
  ** Bereche aus diesen Daten die Schiefe
 * sim3s, stepweite einstellbar machen (ok)
 
28.05.2015
 * 3s: Das ganze Gedönse mit resim etc in extra-Methode verschieben, wo ich teste, welche der Kombis tatsächlich simuliert werden müssen
 * Zu den Referenzdaten: Zur Bestimmung der Schiefen, das muss eh per Hand ausgewählt werden, was das sinnvoll ist. Mich würde interessieren, wie breit so ein Gesamt-Peak ist, wenn ich alle dazugehörigen Spalten zusammen rechne.
 * In den Referenzdaten die IQR berechnen, gleiches für die 2p-Sim, vergleichen, ob das evtl bessere Ergebnisse liefert
 * PAA weiter machen
 * Ich bin mir nicht ganz sicher, ob meine Daten geeignet vorliegen, um den IQR zu berechnen. Wahrscheinlich sind die Sim-Daten richtig, aber die Ref-Daten müsste ich erst umwandeln. Ja, müssen umgewandelt werden
 * Was ich jetzt nicht verstehe: Habe einen Peak kumuliert und dabei einmal etwas Rauschen links und rechts mitgenommen, sodass der Peak insgesamt weniger hoch wurde als beim anderen Versuch, wo ich enger abgeschnitten habe. Komischerweise weist der Peak mit zuviel nebendran einen längeren Tail und größeren IQR auf. Verstehe ich nicht. Könnte Zufall sein, denn das Wertehistogramm ist nur geringfügig unterschiedlich, aber da macht es einen Unterschied, da das Maximum verschoben ist. Durch den längeren Tail ergibt sich natürlich auch ein größerer IQR. Mit anderem Peak noch mal probieren
 * in der 2p-sim: normalisierung ändern, dass verschieden lange säulen auch zum tragen kommen. (am ende, das passt irgendwie nicht) ok
 * Sinnvoll wäre ein Invalid-Flag für abgebrochene Simulationen, damit diese nicht mehr in den Plots auftauchen oder irgendwie mitausgewertet werden. Aktuell gibt es bei der event-sim (2p) noch keine zeit-abbruch bedingung, fraglich, ob das nicht vielleicht sinnvoll ist, es so zu lassen... Evtl koennte fuer alle invalids bei der by-step einfach nachher die event genutzt werden, um zum Ende zu führen 
 * Abbruchtest anpassen auf Länge (ok)
 * Bei den PAA (sowohl mosdi, als auch julia) kommt bei manchen Kombis ein recht "hoher" Wert heraus am ersten Zeitpunkt, der nicht null ist, danach beginnt der normale Peak, diesen Effekt bekomme ich bei den Simulationen nicht. Keine Ahnung, woran das liegt. -> Auch in der Sim, nur durch die bins fällt das dort nicht auf.
 * PAA3s kommentieren, testen, aufräumen. In mosdi Ergebnisse erhalten, Seriensimulationen erstellen.
 * Für Donnerstag Graphiken vorbereiten
 
18.06.2015
 * Mail an Salome wegen robuster Maße ok
 * Dommi nach 16er Gemisch fragen/ 6er per Hand die Daten berechnen
 * Latex: Modern CV für Lebenslauf
 * csv-Tabelle mit Parametern (zum sortieren) sowie Peakdaten (erst mal Loc und IQR, später noch Schiefe) -> Herausfinden sinnvoller Parameterkombis für nicht zu breite aber tailende Peaks zu allen Zeitpunkten
 * Segfault bei np.argmax?! -> jetzt sind keine Strings mehr, sondern floats im Array und es funktioniert wieder
 * Warum habe ich auch 0en am Ende der csv-Dateien? + Besser abspeichern, sodass index auch genutzt wird.
 * argmax zur Bestimmung des Peakmaximums. Restliche Peakdaten???
 * Wie wandle ich meine Wahrscheinlichkeiten in Daten um, aus denen ich Mittelwert, Varianz, IQR etc berechnen kann?
 * In der pd-Tabelle sind noch einige Max bei 0.0999, das macht keinen Sinn -> doch, das sind die mit anfangsausschlag und nachfolgendem Tailing
 * Bei dem Koeffizienten "Gute" Schiefewerte rausfinden
 * Was genau will ich bei den Plots? -> Komprimiere Sim-Daten (zb faktor 100), speichere das ganze wieder als .p ab, inklusive peakdaten, plot geeignet verschieben (offset), beschriftung passend, inkl pd
 * wenn ich den index verschoben habe, muss auf jeden fall eine null ins result, weil für den fall, dass die länge zwar schon auf 1000 ist, habe ich den kleinsten wert aber schon rausgeschmissen, es wäre also falsch, einen anderen Wert auch noch zu löschen. Warum pop! beim berechnen der resultwerte? berechne ich den index falsch? -> Hatte mich mit vorne und hinten vertan, was den index angeht. Ist alles richtig, so wie es ist
 
25.06.2015
 * Zum Aufschreiben: Neues Kapitel "Methoden", wo dann die Sachen reinkommen, die ich bisher in die Implementierung gesteckt hätte. Also Graphiken, wie was berechnet wird, Formel dazu, die man dann einfach runter implementieren kann. Ins implementierungskapitel kommt dann eher kurz welche Sprache, welche Abhängigkeiten (zb Scipy), Überblick, welche Klasse was tut, evtl. welche Methode wo implementiert ist.
 * Plot zum Zusammenhang von Parametern und Peakdaten: Plotte für bestimmten Zeitpunkt auf den Achsen Breite und Schiefe, jeweils Punkte für die Parameter, Punkte beschriften. Das wird zwar evtl unübersichtlich wegen der vielen Parameter, ist aber trotzdem sinnvoll. -> Vergleich möglich mit 2p-Modell, wo ja kaum ein Zeitpunkt überhaupt Schiefe hat. 
 * Nachbearbeitung der julia-sim aufbereiten, *.p anlegen
 * Berechne Peakdaten auf den originaldaten, Komprimiere erst danach. Dann hab ich die richtigen Werte und "Einheiten" drin stehen, kann aber später für's Plotten die komprimierten Daten verwenden
 * Noch so ne spontane Idee: Wenn ich eine gute PAA-Sim gefunden habe, diese auch noch mal mit der Teilchensim laufen lassen
 
09.07.2015
 * Alle Verweise auf die Referenzdaten müssen raus. Eher vage Formulierungen verwenden: "Angenommen es existiert ein Peak mit folgenden Eigenschaften, dann könnte man ihn so simulieren" Es existiert zwar ein öffentlicher Datensatz (in nem paper verwendet) aber der ist wohl nicht gut. 
 Statt dessen eher aufzeigen, welches die Grenzen der Modelle sind, also auch hypotetische Peaks, die es tatsächlich nicht so gibt und auch nicht simulierbar sind.
 * Plots: 
  **Je 3 Params festhalten, den vierten verändern und dann Plotten, wie sich die Peaks verändern, um Zusammenhänge zwischen Parametern und PD zu verdeutlichen (ok)
  ** Zu bestimmten Zeitpunkt eine Fläche füllen, welche Breite/Schiefe erreichbar sind (quasi, so, wie jetzt, nur ohne drangeschriebene Params) dann evtl einige Randpunkte kennzeichnen und deren Params annotieren. Daraus könnte man dann erschließen, wie man die verändern muss, um Peaks innerhalb der Fläche zu erreichen
  ** Im bisherigen Plot der Tabelle die Punkte nicht mit Params beschriften, sondern verschiedene Farben und Formen verwenden und Legende verwenden? Bin aber nicht sicher, ob das übersichtlicher wird.
 * 3festeParams-Plot: Sinnvolle Parameterübergabe für die Methode? Will ja jeden Param prinzipiell variabel machen. Muss also sagen, welcher der variable ist und brauche für diesen eine liste mit möglichen Werten und für die anderen drei jeweils den festen wert angeben. Evtl einfach den Namen als String angeben und den dann aus einer Liste streichen. Brauche aber die richtige Zuordnung, um die passenden Simulationen aufrufen zu können
 * Irgendwas scheint falsch zu sein, die Maxzeitpunkte sind irgendwie unlogisch. Überprüfung mit mosdi oder 3s-sim
 * Habe eine der fraglichen pkombis neu simuliert, jetzt passt der wert. muss wohl teilweise zu alte sims drin haben.
 * Hängt evtl mit dem offset zusammen, evtl beachte ich diesen fälschlicherweise im Moment gar nicht. Die alten sim haben keinen offseteintrag, da stimmen aber die peakdaten. Wenn ich auf das max der neueren sim noch den offset draufaddiere, könnte es auch passen. Seltsam ist auch, dass ich den offset momentan noch durch 10000 teile, der compfactor aber bei nur 1000 liegt, das kann nicht passen. Vorgehensweise: Bei einer Reihe noch mal die originaldaten laden, den offset und compfactor gleich setzen und zum maximum aufaddieren. gucken, ob sich dann eine gleichmäßige reihe ergibt.
 -> passt, berechne jetzt alles neu
 * und schon wieder was merkwürdiges. habe den pll verändert und festgestellt, dass bei steigendem pll (0.99999 statt 0.9999) der Maxzeitpunkt zurück geht. sollte eigentlich nicht so sein, denke ich. seltsamerweise wandert der auch vorwärts, bis zu nem punkt, da springt er zurück und bleibt da. teste jetzt erst mal mit der alten sim, ansonsten böte es sich noch an, zwischenpunkte zu nehmen, gucken, was da so passiert. Auch das Problem liegt leider am offset, neusimulation ohne offset sondern mit nullen bringt ergebnisse, die mit den teilchensimulationen konsistent sind.
 * Habe die Berechnung der Peakdaten jetzt mehrfach überprüft und dabei noch einen anderen minifehler entdeckt, aber beim anschauen der Dateien habe ich das gefühl, der offset an sich stimmt schon nicht. 
 Ich glaube, ich habe den Fehler: Da werden auch am Ende nullen reingepusht, bzw, die zeitverschiebung erhöht?!?, was dann als offset draufaddiert wird ? Ja, das war der Fehler
  